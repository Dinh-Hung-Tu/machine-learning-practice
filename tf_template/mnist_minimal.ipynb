{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimalist TensorFlow implementation of fully connected neural network\n",
    "Some takeaways\n",
    "* Associate a variable with a name. And likewise for `name_scope` of a group of nodes\n",
    "* Initialize weight by truncated normal\n",
    "* Pass the activation function as `tf.nn.relu`\n",
    "* `if tf... is not None`\n",
    "* Output layer is from a fully connected layer of num_output as number of neurons\n",
    "* Keep input as batch so define with flexible size (None, INPUT_SIZE)\n",
    "* What does the session run? It runs the training op, which is to minimize the optimisation objective. \n",
    "* Output can be \n",
    "    * one-hot-code value: `tf.placeholder(tf.int64, shape=(None, OUTPUT_SIZE), name=\"y\")`\n",
    "    * a scalar value: `tf.placeholder(tf.int64, shape=(None), name=\"y\")`\n",
    "* More layers with less number of weights can produce similar performance. But they tend to be more overfitting (training error ~ 0, while test error large)\n",
    "* How values of weights remain from one iteration to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d071576/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `mnist.train` is a DataSet object, supporting the generator to throw next batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple fully connected networks (define a computation flow)\n",
    "* Create placeholders for input X and output y\n",
    "* Two hidden layers\n",
    "* One softmax layer at the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 28\n",
    "INPUT_SIZE = IMAGE_WIDTH*IMAGE_WIDTH\n",
    "OUTPUT_SIZE = 10 # 10 digits\n",
    "n_hidden_1 = 100  # Subject to change, test\n",
    "n_hidden_2 = 50\n",
    "X = tf.placeholder(tf.float32, shape=(None, INPUT_SIZE), name=\"X\")  # Using \"name\" to show in graph\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a ReLU activation function\n",
    "* For plotting, if needed. But not as an activation function for tensorflow because it is not `native`, missing some requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"Work for both scalar and vector. But not compatible with TensorFlow\"\"\"\n",
    "    return np.maximum(z,0)\n",
    "\n",
    "assert relu(10) == 10\n",
    "assert relu(-10) == 0\n",
    "assert np.array_equal(relu(np.array([10, -10])),np.array([10, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a low-level neural network layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_layer(X, num_neurons, name, activation = None):\n",
    "    \"\"\"Input: \n",
    "        - X: Input vector to the layer\n",
    "        - size of the layer (number of neurons)\n",
    "        - name of the layer\n",
    "        - activation function on top, before outputing\n",
    "        Return: \n",
    "        - output_vector\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name): # Name scope to group related nodes\n",
    "        # Get the size of each input\n",
    "        n_inputs = int(X.get_shape()[1])  # It could be in batch dim(X) = 5x748 (every 5 pictures go together)\n",
    "        \n",
    "        # Define the weight matrix, initialized randomly with Gausian distribution, std dev = 2/sqrt(n_inputs)\n",
    "        stddev = 2/np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, num_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        \n",
    "        # Define bias\n",
    "        b = tf.Variable(tf.zeros([num_neurons]), name=\"bias\")\n",
    "        \n",
    "        # Get the output\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        \n",
    "        # Activation is optional, if present, they can be ReLU or sigmoid function\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option for a higher level (wrapper) of the single layer\n",
    "`tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack the layers together in a first multi-layer neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"): # Deep neural networks\n",
    "    hidden_1 = neural_layer(X, n_hidden_1, name=\"hidden_1\", activation=tf.nn.relu)\n",
    "    hidden_2 = neural_layer(hidden_1, n_hidden_2, name=\"hidden_2\", activation=tf.nn.relu)\n",
    "    logits = neural_layer(hidden_2, OUTPUT_SIZE, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"): # Define the loss function for optimisation later\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimiser to minimize the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"): # Objective of training is to minimize loss function\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)  # Define a training operation as the process of minimizing the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"evaluate\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)  # compare true labels y with the prediction, `in_top_k` takes care of argmax\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution phase (put the computation flow in action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \n",
      "\tTrain accuracy: 0.92 \n",
      "\tTest accuracy: 0.9078\n",
      "Epoch:  5 \n",
      "\tTrain accuracy: 0.94 \n",
      "\tTest accuracy: 0.9482\n",
      "Epoch:  10 \n",
      "\tTrain accuracy: 0.98 \n",
      "\tTest accuracy: 0.9595\n",
      "Epoch:  15 \n",
      "\tTrain accuracy: 0.98 \n",
      "\tTest accuracy: 0.9656\n",
      "Epoch:  20 \n",
      "\tTrain accuracy: 0.98 \n",
      "\tTest accuracy: 0.9702\n",
      "Epoch:  25 \n",
      "\tTrain accuracy: 0.98 \n",
      "\tTest accuracy: 0.971\n",
      "Epoch:  30 \n",
      "\tTrain accuracy: 0.98 \n",
      "\tTest accuracy: 0.9729\n",
      "Epoch:  35 \n",
      "\tTrain accuracy: 1.0 \n",
      "\tTest accuracy: 0.9727\n",
      "Epoch:  39 \n",
      "\tTrain accuracy: 1.0 \n",
      "\tTest accuracy: 0.9743\n"
     ]
    }
   ],
   "source": [
    "# Set some administrative parameters\n",
    "init_variables = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "num_samples = mnist.train.num_examples\n",
    "n_iterations = num_samples//batch_size\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # First init all variables\n",
    "    init_variables.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(n_iterations):\n",
    "            # Get batch\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict = {X:X_batch, y:y_batch})\n",
    "        acc_test  = accuracy.eval(feed_dict = {X:mnist.test.images, y:mnist.test.labels})\n",
    "        if epoch%5==0 or epoch==n_epochs-1: \n",
    "            summary_str = loss_summary.eval(feed_dict={X: X_batch, y: y_batch})  # Record training loss\n",
    "            step = epoch * batch_size + iteration\n",
    "            file_writer.add_summary(summary_str, step)\n",
    "            print(\"Epoch: \", epoch, \"\\n\\tTrain accuracy:\", acc_train, \"\\n\\tTest accuracy:\", acc_test)\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualisation \n",
    "TensorBoard can provide the following capability \n",
    "* Visualise the computation graph\n",
    "* Train vs test error as a function of epoch\n",
    "\n",
    "**In code** it should be 3 steps:\n",
    "* Create the log writer\n",
    "```\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "```\n",
    "* Log the relevant information at certain time step\n",
    "```\n",
    "if epoch%5==0 or epoch==n_epochs-1: \n",
    "    summary_str = loss_summary.eval(feed_dict={X: X_batch, y: y_batch})  # Record training loss\n",
    "    step = epoch * batch_size + iteration\n",
    "    file_writer.add_summary(summary_str, step)\n",
    "```\n",
    "* Close the writer `file_writer.close()`\n",
    "\n",
    "**In command line**, run the following `$tensorboard --logdir [path_to_log_dir]`\n",
    "\n",
    "Check the result in the address: http://localhost:6006/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computation flow**\n",
    "![](img/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train loss vs epochs**\n",
    "![Train vs epochs](img/train_vs_epochs.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
